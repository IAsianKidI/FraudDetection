{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "r7k5qdyHLFSd"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "2XbHCQKSLREn"
      },
      "outputs": [],
      "source": [
        "# Feedforward Neural Network\n",
        "class FNN():\n",
        "  # this method is a constructor and is used to create an instance of the feedforward neural network model containing the training data\n",
        "  # parameters\n",
        "  # learning_rate: this contains a float which is used to change how big of a jump the data will make\n",
        "  # epoch: this contains a integer which is used to repeat the training epoch times\n",
        "  # x: this contains the training dataset examples\n",
        "  # y: this contains the training dataset answers\n",
        "  def __init__(self, learning_rate, epoch, x, y):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.epoch = epoch\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "    self.number_data, self.number_feature = x.shape\n",
        "    self.input_weights = np.zeros((self.number_feature, 4))\n",
        "    self.bias = 1\n",
        "    self.bias_hidden = 1\n",
        "    self.bias_output = 1\n",
        "    self.hidden_weights = np.zeros(4)\n",
        "    self.hidden_weights2 = np.zeros((4, 4))\n",
        "\n",
        "  # this method is used for back propagation\n",
        "  # parameters\n",
        "  # input_hidden2: this contains an array that contains the input to hidden values but with relu applied to it\n",
        "  # hidden2_hidden: this contains an array that contains the hidden to hidden values but with relu applied to it\n",
        "  # output_output: this contains an array that contains the hidden to output values but with sigmoid applied to it\n",
        "  def backward_pass(self, input_hidden2, hidden2_hidden,hidden_output):\n",
        "    # Calculate errors\n",
        "    # x\n",
        "    output_error = hidden_output - self.y\n",
        "    # x by 4\n",
        "    hidden_error = np.outer(output_error, self.hidden_weights)\n",
        "    # x by 4\n",
        "    hidden_error2 = np.dot(hidden_error, self.hidden_weights2)\n",
        "\n",
        "    # Calculating gradients. x by 4\n",
        "    output_weights_gradient = (1/self.number_data) * np.dot(hidden2_hidden.T, output_error)\n",
        "    output_grad_bias = (1/self.number_data) * np.sum(output_error)\n",
        "\n",
        "    # x by 4\n",
        "    hidden_weights_gradient = (1/self.number_data) * np.dot(input_hidden2.T, hidden_error)\n",
        "    hidden_grad_bias = (1/self.number_data) * np.sum(hidden_error)\n",
        "\n",
        "    # 4 by features\n",
        "    hidden_weights_gradient2 = (1/self.number_data) * np.dot(self.x.T, hidden_error2)\n",
        "    hidden_grad_bias2 = (1/self.number_data) * np.sum(hidden_error2)\n",
        "\n",
        "    # Updating weights and biases\n",
        "    self.hidden_weights -= self.learning_rate * output_weights_gradient\n",
        "    self.bias_output -= self.learning_rate * output_grad_bias\n",
        "\n",
        "    self.hidden_weights2 -= self.learning_rate * hidden_weights_gradient\n",
        "    self.bias_hidden -= self.learning_rate * hidden_grad_bias\n",
        "\n",
        "    self.input_weights -= self.learning_rate * hidden_weights_gradient2\n",
        "    self.bias -= self.learning_rate * hidden_grad_bias2\n",
        "\n",
        "\n",
        "  # trains the model\n",
        "  def fit(self):\n",
        "    for p in range(self.epoch):\n",
        "\n",
        "      # input to hidden. x by 4\n",
        "      hidden_input = np.dot(self.x, self.input_weights) + self.bias\n",
        "      # x by 4\n",
        "      relu_activation = np.maximum(0, hidden_input)\n",
        "\n",
        "      # hidden2 to hidden. x by 4\n",
        "      hidden_input2 = np.dot(relu_activation, self.hidden_weights2) + self.bias_hidden\n",
        "      # x by 4\n",
        "      relu_activation2 = 1 / (1 + np.exp(-hidden_input2))\n",
        "\n",
        "      # hidden to output using sigmoid since it is a binary classification. x\n",
        "      output_hidden = np.dot(relu_activation2, self.hidden_weights) + self.bias_output\n",
        "      # x\n",
        "      Sigmoid = 1 / (1 + np.exp(-output_hidden))\n",
        "      # gradiants\n",
        "      self.backward_pass(relu_activation, relu_activation2, Sigmoid)\n",
        "\n",
        "\n",
        "  # finds the accuracy with the testing dataset\n",
        "  # parameters\n",
        "  # x_test: contains the testing dataset examples\n",
        "  # y_test: contains the testing dataset answers\n",
        "  def accuracy(self, x_test, y_test):\n",
        "    # calculate the outputs\n",
        "    hidden_input = np.dot(x_test, self.input_weights) + self.bias\n",
        "    relu_activation = np.maximum(0, hidden_input)\n",
        "\n",
        "    # hidden2 to hidden. x by 4\n",
        "    hidden_input2 = np.dot(relu_activation, self.hidden_weights2) + self.bias_hidden\n",
        "    # x by 4\n",
        "    relu_activation2 = 1 / (1 + np.exp(-hidden_input2))\n",
        "\n",
        "    # hidden to output\n",
        "    output_hidden = np.dot(relu_activation2, self.hidden_weights) + self.bias_output\n",
        "    Sigmoid = 1 / (1 + np.exp(-output_hidden))\n",
        "\n",
        "    Is_Fraud = 0\n",
        "    for i in range(len(y_test)):\n",
        "      if y_test[i] == 1:\n",
        "        Is_Fraud += 1\n",
        "\n",
        "    print(Is_Fraud)\n",
        "    accuracy = 0\n",
        "    TP = 0\n",
        "    FP = 0\n",
        "    TN = 0\n",
        "    FN = 0\n",
        "    # finds accuracy\n",
        "    for i in range(len(Sigmoid)):\n",
        "      # uses the threshold to determine if it is 1 or 0\n",
        "      if Sigmoid[i] > .5:\n",
        "        print(\"Hi\")\n",
        "        # checks if the guess was correct, if it was increment by 1\n",
        "        if y_test[i] == 1:\n",
        "          accuracy += 1\n",
        "          TP += 1\n",
        "        else:\n",
        "          FP += 1\n",
        "      else:\n",
        "        # checks if the guess was correct, if it was increment by 1\n",
        "        if y_test[i] == 0:\n",
        "          accuracy += 1\n",
        "          TN += 1\n",
        "        else:\n",
        "          FN += 1\n",
        "    print(TP)\n",
        "    print(FP)\n",
        "    print(FN)\n",
        "    f1 = TP / (TP + ((1/2) * (FP + FN)))\n",
        "    # returns the percentage\n",
        "    return (accuracy / len(y_test)) * 100, f1\n",
        "  \n",
        "  # this method will save the weights of the model into a python file\n",
        "    def saveWeights(self):\n",
        "        # this is a data type that stores key pair values so it stores w1 with the w1 weights\n",
        "        save = {\n",
        "            \"w1\": self.w1,\n",
        "            \"w2\": self.w2\n",
        "        }\n",
        "\n",
        "        # opens the json file in write mode and closes it when finished\n",
        "        with open(\"save.json\", \"w\") as f:\n",
        "            # stores save in the json file\n",
        "            json.dump(save, f)\n",
        "\n",
        "\n",
        "    # this method will load the weights of the model into the code\n",
        "    def loadWeights(self):\n",
        "        # opens the json file in read mode and closes it when finished\n",
        "        with open(\"save.json\", \"r\") as f:\n",
        "            # loads what is stored in the json file into the save\n",
        "            save = json.load(f)\n",
        "            # set the weights stored in save\n",
        "            self.w1 = save[\"w1\"]\n",
        "            self.w2 = save[\"w2\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_data(filename):\n",
        "    df = pd.read_csv(filename)\n",
        "    x = df.drop(columns=['merchant','first','last','gender','street','city','state','zip','job','dob','trans_num','unix_time','trans_date_trans_time','cc_num'])\n",
        "    # y = df['is_fraud']\n",
        "    dataset=x\n",
        "    # dataset = np.column_stack((x, y))\n",
        "    #np.random.shuffle(dataset)\n",
        "    # Split data\n",
        "    train_size = int(0.7 * len(dataset))\n",
        "    train_dataset, test_dataset = dataset[:train_size], dataset[train_size:]\n",
        "    #features\n",
        "    train_dataset=np.delete(train_dataset,0,axis=1)\n",
        "    test_dataset=np.delete(test_dataset,0,axis=1)\n",
        "    #targets\n",
        "    train_target=train_dataset[:,7]\n",
        "    test_target=test_dataset[:,7]\n",
        "\n",
        "    train_dataset=np.delete(train_dataset,7,axis=1)\n",
        "    test_dataset=np.delete(test_dataset,7,axis=1)\n",
        "\n",
        "    return train_dataset, test_dataset, train_target, test_target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1776\n",
            "389003\n",
            "369\n",
            "166716\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "could not convert string to float: 'personal_care'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[62], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(Is_Fraud)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(test_target))\n\u001b[1;32m---> 20\u001b[0m train_sample_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m test_sample_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(test_sample)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Feedforward Neural Networks running\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1084\u001b[0m             (\n\u001b[0;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1094\u001b[0m         )\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\preprocessing\\_data.py:876\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 876\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\preprocessing\\_data.py:912\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[0;32m    881\u001b[0m \n\u001b[0;32m    882\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    911\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 912\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    913\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    914\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    919\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    921\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:997\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    995\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 997\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    998\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1000\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m   1001\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_array_api.py:521\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    519\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 521\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    523\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
            "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'personal_care'"
          ]
        }
      ],
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "#output get_data\n",
        "train_sample,test_sample,train_target,test_target=get_data('fraudTest.csv')\n",
        "Is_Fraud = 0\n",
        "for i in range(len(train_target)):\n",
        "    if train_target[i] == 1:\n",
        "        Is_Fraud += 1\n",
        "\n",
        "print(Is_Fraud)\n",
        "print(len(train_target))\n",
        "\n",
        "Is_Fraud = 0\n",
        "for i in range(len(test_target)):\n",
        "    if test_target[i] == 1:\n",
        "        Is_Fraud += 1\n",
        "\n",
        "print(Is_Fraud)\n",
        "print(len(test_target))\n",
        "train_sample_scaled = scaler.fit_transform(train_sample)\n",
        "test_sample_scaled = scaler.transform(test_sample)\n",
        "\n",
        "# Feedforward Neural Networks running\n",
        "print(\"-----------------------------------FNN-----------------------------------\")\n",
        "Feedforward_Neural_Network = FNN(0.1, 100, train_sample_scaled, train_target)\n",
        "Feedforward_Neural_Network.fit()\n",
        "accuracy, f1 = Feedforward_Neural_Network.accuracy(test_sample_scaled, test_target)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"F1 Score:\", f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow.python'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[60], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LSTM, Dense\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\__init__.py:37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msite\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_site\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.python'"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'Sequential' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[61], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m X_test_reshaped \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(test_sample_scaled, (test_sample_scaled\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m, test_sample_scaled\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# creates the model\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSequential\u001b[49m()\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39madd(LSTM(\u001b[38;5;241m4\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m7\u001b[39m)))\n\u001b[0;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m1\u001b[39m))\n",
            "\u001b[1;31mNameError\u001b[0m: name 'Sequential' is not defined"
          ]
        }
      ],
      "source": [
        "# reshape so it can match the input shape of the model\n",
        "X_train_reshaped = np.reshape(train_sample_scaled, (train_sample_scaled.shape[0], 1, train_sample_scaled.shape[1]))\n",
        "X_test_reshaped = np.reshape(test_sample_scaled, (test_sample_scaled.shape[0], 1, test_sample_scaled.shape[1]))\n",
        "\n",
        "# creates the model\n",
        "model = Sequential()\n",
        "model.add(LSTM(4, input_shape=(1, 7)))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# compiles the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# trains the model\n",
        "history = model.fit(X_train_reshaped, train_target, epochs=1000, verbose=2)\n",
        "\n",
        "# evaluates the model\n",
        "loss = model.evaluate(X_test_reshaped, test_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the model we are using\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "smote = SMOTE(random_state=1)\n",
        "train_sample_resampled, train_target_resampled = smote.fit_resample(train_sample, train_target)\n",
        "# Instantiate model with 1000 decision trees\n",
        "rf = RandomForestRegressor(n_estimators = 1000, random_state = 1)\n",
        "# Train the model on training data\n",
        "rf.fit(train_sample_resampled, train_target_resampled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "y_pred = rf.predict(test_sample)\n",
        "\n",
        "accuracy = 0\n",
        "# finds accuracy\n",
        "for i in range(len(y_pred)):\n",
        "  # uses the threshold to determine if it is 1 or 0\n",
        "  if y_pred[i] > .5:\n",
        "    # checks if the guess was correct, if it was increment by 1\n",
        "    if test_target[i] == 1:\n",
        "      accuracy += 1\n",
        "  else:\n",
        "    # checks if the guess was correct, if it was increment by 1\n",
        "    if test_target[i] == 0:\n",
        "      accuracy += 1\n",
        "\n",
        "accuracy = (accuracy / len(test_target)) * 100\n",
        "\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
